# 大数据最新面试题，2021年面试题及答案汇总

### 其实，博主还整理了，更多大厂面试题，直接下载吧

### 下载链接：[高清172份，累计 7701 页大厂面试题  PDF](https://github.com/souyunku/DevBooks/blob/master/docs/index.md)



### 1、insert into 和override write区别

insert into：将某一个表的数据写到另一个表

override write ：覆盖之前的表


### 2、给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

方案1：可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

遍历文件a，对每个url求取hash(url)00，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。

遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。

求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。

方案2：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。

Bloom filter日后会在本BLOG内详细阐述。


### 3、描述Hbase，ZooKeeper搭建过程

hadoop运行原理

hadoop的主要核心是由两部分组成，HDFS和mapreduce，首先HDFS的原理就是分布式的文件存储系统，将一个大的文件，分割成多个小的文件，进行存储在多台服务器上。

Mapreduce的原理就是使用JobTracker和TaskTracker来进行作业的执行。Map就是将任务展开，reduce是汇总处理后的结果。


### 4、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

方案：顺序读文件中，对于每个词x，取hash(x)P00，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。

如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。

对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100个词及相应的频率存入文件，这样又得到了5000个文件。下一步就是把这5000个文件进行归并（类似与归并排序）的过程了。


### 5、如何确定hadoop集群的健康状态

UI监控 shell监控


### 6、简述hadoop spark storm hive的特点及使用场景

hadoop是一种分布式系统基础架构当处理海量数据对的程序，开始要求高可靠，高扩展，高效，低容错，低成本场景

MapReduce是一种编程模型，用于大规模数据集的并行计算，目前日志分析居多

spark拥有mr的所具有的优点；但不同于mr的是job中间输出的结果可以保存到内存中，从而不需要读写hdfs，由此spark能更好的适用于数据挖掘与机器学习等需要迭代式计算，极大的提高效率的场景

storm ：一个分布式实时计算系统storm是一个任务并行连续计算引擎，storm并不在hadoop集群运行，他是用Zookeeper的和自己的 主从 工作进程，协调拓扑和工作者状态

hive 数据仓库

hbase：数据量大，传统数据库无法胜任，联机业务功能开发，离线数据分析


### 7、请列出你所知道的hadoop调度器，并简要说明其工作方法

Fifo schedular :默认，先进先出的原则

Capacity schedular :计算能力调度器，选择占用最小、优先级高的先执行，依此类推。

Fair schedular:公平调度，所有的 job 具有相同的资源。


### 8、搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。
### 9、Hbase的rowKey怎么创建比较好？列簇怎么创建比较好？
### 10、Kafka的消息发送
### 11、全分布式环境下为什么需求password-less SSH？
### 12、hadoop 的 namenode 宕机怎么解决
### 13、怎么样才能实现去掉reduce阶段
### 14、列族怎么创建比较好 <=2
### 15、hbase如何调优
### 16、请描述如何解决Hbase中region太小和region太大带来的结果。
### 17、简答说一下hadoop的map-reduce编程模型
### 18、我们在开发分布式计算job的时候，是否可以去掉reduce阶段
### 19、HBase写数据的原理是什么？
### 20、Spark为什么要持久化，一般什么场景下要进行persist操作？
### 21、请描述一下开发过程中如何对上面的程序进行性能分析，对性能分析进行优化的过程。
### 22、如何退出输入模式？
### 23、三个datanode中当有一个datanode出现错误时会怎样？
### 24、varhadooppids用于做什么？
### 25、生产环境为什么建议使用外部表
### 26、举一个例子说明mapreduce是怎么运行的。
### 27、hadoop和spark都是并行计算，那么他们有什么相同和区别？
### 28、Hive生产环境中为什么建议使用外部表？
### 29、谈谈数据倾斜，如何发生的，并给出相应的解决办法
### 30、这会导致安全问题吗？
### 31、Kafka与传统消息队列的区别
### 32、hive与传统DB的区别




## 全部答案，整理好了，直接下载吧

### 下载链接：[全部答案，整理好了](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin-2.png)




## 最新，高清PDF：172份，7701页，最新整理

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/mst.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")
