# 大数据最新2021年面试题及答案，汇总版

### 其实，博主还整理了，更多大厂面试题，直接下载吧

### 下载链接：[高清172份，累计 7701 页大厂面试题  PDF](https://www.souyunku.com/?p=67)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/?p=67)



### 1、请说明hive中Sort By、Order By、Cluster By，Distribute By各代表什么意思？

order by：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。

sort by：不是全局排序，其在数据进入reducer前完成排序。

distribute by：按照指定的字段对数据进行划分输出到不同的reduce中。

cluster by：除了具有 distribute by 的功能外还兼具 sort by 的功能。


### 2、描述Hbase，ZooKeeper搭建过程

hadoop运行原理

hadoop的主要核心是由两部分组成，HDFS和mapreduce，首先HDFS的原理就是分布式的文件存储系统，将一个大的文件，分割成多个小的文件，进行存储在多台服务器上。

Mapreduce的原理就是使用JobTracker和TaskTracker来进行作业的执行。Map就是将任务展开，reduce是汇总处理后的结果。


### 3、简单说一下hadoop的map-reduce模型

首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合，使用的是hadoop内置的数据类型，如Text，Longwritable等。

将键值对集合输入mapper进行业务处理过程，将其转化成需要的key-value再输出。

之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getPartition方法来自定义分区规则。

之后会对key进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则。

之后进行一个combiner归约操作，就是一个本地的reduce预处理，以减小shuffle，reducer的工作量。

Reduce task会用过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job。


### 4、hive的原数据存储

hive中的原数据存放在关系型数据库中 MySQL、derby中，元数据包括 表的名字，表的列和分区以及属性，标的属性是否是外部表，标的数据所在目录


### 5、腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

与上第6题类似，我的第一反应时快速排序+二分查找。以下是其它更好的方法：

方案1：oo，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

方案2：这个问题在《编程珠玑》里有很好的描述，大家可以参考下面的思路，探讨一下：

又因为2^32为40亿多，所以给定一个数可能在，也可能不在其中；

这里我们把40亿个数中的每一个用32位的二进制来表示

假设这40亿个数开始放在一个文件中。

然后将这40亿个数分成两类:

1.最高位为0

2.最高位为1

并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20亿，而另一个>=20亿（这相当于折半了）；

与要查找的数的最高位比较并接着进入相应的文件再查找

再然后把这个文件为又分成两类:

1.次最高位为0

2.次最高位为1

并将这两类分别写入到两个文件中，其中一个文件中数的个数<=10亿，而另一个>=10亿（这相当于折半了）；

与要查找的数的次最高位比较并接着进入相应的文件再查找。

…….

以此类推，就可以找到了,而且时间复杂度为O(logn)，方案2完。

附：这里，再简单介绍下，位图方法：

使用位图法判断整形数组是否存在重复

判断集合中存在重复是常见编程任务之一，当集合中数据量比较大时我们通常希望少进行几次扫描，这时双重循环法就不可取了。

位图法比较适合于这种情况，它的做法是按照集合中最大元素max创建一个长度为max+1的新数组，然后再次扫描原数组，遇到几就给新数组的第几位置上1，如遇到5就给新数组的第六个元素置1，这样下次再遇到5想置位时发现新数组的第六个元素已经是1了，这说明这次的数据肯定和以前的数据存在着重复。这种给新数组初始化时置零其后置一的做法类似于位图的处理方法故称位图法。它的运算次数最坏的情况为2N。如果已知数组的最大值即能事先给新数组定长的话效率还能提高一倍。

欢迎，有更好的思路，或方法，共同交流。


### 6、Redis，传统数据库，hbase，hive每个之间的区别

Redis是缓存

hbase是列式数据库，存在hdfs上，写入速度快，数据量大，查询速度快

hive是数据仓库，是用来分析数据，不是增删改查数据的


### 7、请简述mapreduce中的combine和partition的作用

combiner是发生在map的最后一个阶段，其原理也是一个小型的reducer，主要作用是减少输出到reduce的数据量，缓解网络传输瓶颈，提高reducer的执行效率。

partition的主要作用将map阶段产生的所有kv对分配给不同的reducer task处理，可以将reduce阶段的处理负载进行分摊


### 8、什么是spark

基于内存计算发数据分析引擎，提高在大数据环境下数处理的实时性，spark仅涉及数据计算


### 9、当前日志采样格式为

a , b , c , d

b , b , f , e

a , a , c , f


### 10、介绍一下hbase

hbase典型的key/value 系统，建立在hdfs之上，提供高可靠性，高性能，列存储，可伸缩，实时读写nosql的数据库系统。

主要用于海量结构化和半结构化数据存储

hbase查询数据功能很简单，不支持复杂操作，不支持复杂的事务

hbase主要依靠横向扩展


### 11、SSH工作的端口号是？
### 12、如何重启Namenode？
### 13、Kafka怎么保证消息不丢失机制
### 14、hive有哪些保存元数据的方式，各有什么特点
### 15、hive底层与数据库交互原理
### 16、Namenode、Job tracker和task tracker的端口号是？
### 17、spark的优化怎么做？
### 18、有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
### 19、什么是udf
### 20、收集日志的模型
### 21、VM是否可以称为Pseudo？
### 22、单机（本地）模式中的注意点？
### 23、hive如何优化
### 24、是否可以在Windows上运行Hadoop？
### 25、查看所有的topic
### 26、用mapreduce怎么处理数据倾斜问题
### 27、简述hadoop spark storm hive的特点及使用场景
### 28、简单说一下hadoop和spark的shuffle过程
### 29、flush的过程
### 30、3个datanode中有一个datanode出现错误会怎样？
### 31、sqoop在导入数据到MySQL中，如何不重复导入数据，如果存在数据问题，sqoop如何处理？
### 32、hdfs运行原理




## 全部答案，整理好了，直接下载吧

### 下载链接：[全部答案，整理好了](https://www.souyunku.com/?p=67)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/?p=67)


## 最新，高清PDF：172份，7701页，最新整理

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/mst.png "大厂面试题")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png"大厂面试题")

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")
