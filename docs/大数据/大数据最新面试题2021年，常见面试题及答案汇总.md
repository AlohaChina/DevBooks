# 大数据最新面试题2021年，常见面试题及答案汇总

### 其实，博主还整理了，更多大厂面试题，直接下载吧

### 下载链接：[高清172份，累计 7701 页大厂面试题  PDF](https://www.souyunku.com/?p=67)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/?p=67)



### 1、启动和关闭命令会用到哪些文件？

Slaves及Masters。


### 2、Hbase的rowKey怎么创建比较好？列簇怎么创建比较好？

rowKey最好要创建有规则的rowKey，即最好是有序的。

经常需要批量读取的数据应该让他们的rowkey连续；

将经常需要作为条件查询的关键词组织到rowkey中；

列族的创建：

按照业务特点，把数据归类，不同类别的放在不同列族


### 3、请列出正常的hadoop集群中hadoop都分别需要启动 哪些进程，他们的作用分别都是什么，请尽量列的详细一些。

namenode：负责管理hdfs中文件块的元数据，响应客户端请求，管理datanode上文件block的均衡，维持副本数量

Secondname:主要负责做checkpoint操作；也可以做冷备，对一定范围内数据做快照性备份。

Datanode:存储数据块，负责客户端对数据块的io请求

Jobtracker :管理任务，并将任务分配给 tasktracker。

Tasktracker: 执行JobTracker分配的任务。

Resourcemanager

Nodemanager

Journalnode

Zookeeper

Zkfc


### 4、hadoop-metrics.properties文件的作用是？

hadoop-metrics.properties被用做“Reporting”，控制Hadoop报告，初始状态是“not to report”。


### 5、请简述hadoop怎样实现二级排序（就是对key和value双排序）

第一种方法是，Reducer将给定key的所有值都缓存起来，然后对它们再做一个Reducer内排序。但是，由于Reducer需要保存给定key的所有值，可能会导致出现内存耗尽的错误。

第二种方法是，将值的一部分或整个值加入原始key，生成一个组合key。这两种方法各有优势，第一种方法编写简单，但并发度小，数据量大的情况下速度慢(有内存耗尽的危险)，

第二种方法则是将排序的任务交给MapReduce框架shuffle，更符合Hadoop/Reduce的设计思想。这篇文章里选择的是第二种。我们将编写一个Partitioner，确保拥有相同key(原始key，不包括添加的部分)的所有数据被发往同一个Reducer，还将编写一个Comparator，以便数据到达Reducer后即按原始key分组。

68.简述hadoop实现jion的几种方法

Map side join----大小表join的场景，可以借助distributed cache

Reduce side join


### 6、storm特点

编程模型简单，可扩展，高可用性，高容错性，支持多种编程语言，支持本地模式，高效，亚秒级

吞吐量低

nimbus：主控节点，用于提交任务，分配集群任务并架空集群运行状态等，可分配多个nimbus，做高可用

Zookeeper：协调集群，公共数据的存放（如心跳数据，集群的状态和配置信息），nimbus将分配给supervisor的任务写入到Zookeeper

supervisor:负责接收nimbus分配的任务，管理属于自己的worker进程

worker：运行具体处理组件逻辑的进程 ，worker中每一个spout/bolt的线程为一个task

spout：是接收外部数据的组件，将外部数据源转化为Storm内部数据结构，以Tuple为基本的传输单元发给bolt.

tuple:是storm内部数据传输的基本单元，里面封装了一个List对象，用来保存数据

storm进程参数

worker：一个进程

executor：worker启动的线程

task：实际执行数据处理的最小工作单元

并行度主要取决于 Kafka 中topic的数据量，分析topic每个partition的每秒数据量， partition数据量=spoutask数据量 spouttask数量=partition数量 =worker数量


### 7、你认为用java ， streaming ， pipe方式开发map/reduce ， 各有哪些优点

Java 写 mapreduce 可以实现复杂的逻辑，如果需求简单，则显得繁琐。

HiveQL 基本都是针对 hive 中的表数据进行编写，但对复杂的逻辑（杂）很难进行实现。写起来简单。


### 8、Slaves由什么组成？

Slaves由主机的列表组成，每台1行，用于说明数据节点。


### 9、Hadoop安装在什么目录下？

Cloudera和Apache使用相同的目录结构，Hadoop被安装在cdusrlibhadoop-0.20。


### 10、全分布式环境下为什么需求password-less SSH？

这主要因为集群中通信过于频繁，Job Tracker需要尽可能快的给Task Tracker发布任务。


### 11、如何从SU转到Cloudera？
### 12、伪分布模式中的注意点？
### 13、insert into 和override write区别
### 14、Spark为什么要持久化，一般什么场景下要进行persist操作？
### 15、存储特点
### 16、hbase内部机制是什么
### 17、请列出你在工作中使用过的开发mapreduce的语言
### 18、spark调优
### 19、map-reduce程序运行的时候会有什么比较常见的问题
### 20、如何确定hadoop集群的健康状态
### 21、hive是什么
### 22、Hadoop-env.sh文件当下的位置？
### 23、hive sql知识点
### 24、上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。
### 25、hbase如何调优
### 26、RAM的溢出因子是？
### 27、如何知道消费者消费到哪一条消息了？
### 28、如何确认hadoop集群的健康状况
### 29、MapReduce的map数量和reduce数量怎么确定，怎么配置
### 30、List与set的区别
### 31、假设公司要建一个数据中心，你会如何处理？
### 32、fs.mapr.working.dir只是单一的目录？




## 全部答案，整理好了，直接下载吧

### 下载链接：[全部答案，整理好了](https://www.souyunku.com/?p=67)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/?p=67)


## 最新，高清PDF：172份，7701页，最新整理

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/mst.png "大厂面试题")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png"大厂面试题")

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")
