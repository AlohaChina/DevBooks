# 大数据最新2021年面试题，高级面试题及附答案解析

### 其实，博主还整理了，更多大厂面试题，直接下载吧

### 下载链接：[高清172份，累计 7701 页大厂面试题  PDF](https://github.com/souyunku/DevBooks/blob/master/docs/index.md)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png)



### 1、hive如何控制权限

持简单的权限管理，默认情况下是不开启，这样所有的用户都具有相同的权限，同时也是超级管理员，也就对hive中的所有表都有查看和改动的权利，这样是不符合一般数据仓库的安全原则的。Hive可以是基于元数据的权限管理，也可以基于文件存储级别的权限管理，此次以介绍MetaData权限管理为主。通过以下配置开启Hive身份认证功能进行权限检查：


### 2、hive中存放的是什么？

表。

存的是和hdfs的映射关系，hive是逻辑上的数据仓库，实际操作的都是hdfs上的文件，HQL就是用SQL语法来写的MR程序。


### 3、RDD 是什么

弹性分布式数据集，是spark中最基本的数据抽象，可以存于内存中或者磁盘中，分布式存储可用于分布式计算

一个不可变，可分区，里面的数据可并行计算的集合


### 4、给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

方案1：可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

遍历文件a，对每个url求取hash(url)00，然后根据所取得的值将url分别存储到1000个小文件（记为a0,a1,…,a999）中。这样每个小文件的大约为300M。

遍历文件b，采取和a相同的方式将url分别存储到1000小文件（记为b0,b1,…,b999）。这样处理后，所有可能相同的url都在对应的小文件（a0vsb0,a1vsb1,…,a999vsb999）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。

求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。

方案2：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。

Bloom filter日后会在本BLOG内详细阐述。


### 5、hdfs的体系结构

集群架构：

namenode datanode secondarynamenode

(active namenode ,standby namenode)journalnode zkfc

内部工作机制：

数据是分布式存储的

对外提供一个统一的目录结构

对外提供一个具体的响应者（namenode）

数据的block机制，副本机制

Namenode和datanode的工作职责和机制

读写数据流程


### 6、请列出你所知道的hadoop调度器，并简要说明其工作方法

Fifo schedular :默认，先进先出的原则

Capacity schedular :计算能力调度器，选择占用最小、优先级高的先执行，依此类推。

Fair schedular:公平调度，所有的 job 具有相同的资源。


### 7、对于hadoop自带的wordcount的例子，value就是一个叠加的数字，所以map一结束就可以进行reduce的value叠加，而不必要等到所有的map结束再去进行reduce的value叠加。

combiner使用的合适，可以在满足业务的情况下提升job的速度，如果不合适，则将导致输出的结果不正确。


### 8、假如一个分区的数据逐步错误怎么通过hivesql删除

alter table ptable drop partition(daytime=‘20140921’,city=‘bj’);全部删除，文件夹还在


### 9、List与set的区别

List和Set都是接口。他们各自有自己的实现类，有无顺序的实现类，也有有顺序的实现类。

最大的不同就是List是可以重复的。而Set是不能重复的。

List适合经常追加数据，插入，删除数据。但随即取数效率比较低。

Set适合经常地随即储存，插入，删除。但是在遍历时效率比较低。


### 10、Slaves由什么组成？

Slaves由主机的列表组成，每台1行，用于说明数据节点。


### 11、为什么要用flume导入hdfs，hdfs的构架是怎样的
### 12、请列举出曾经修改过的/etc/下面的文件，并说明修改要解决什么问题？
### 13、sqoop在导入数据到MySQL中，如何不重复导入数据，如果存在数据问题，sqoop如何处理？
### 14、怎么设置RDD cache
### 15、hadoop 的 namenode 宕机怎么解决
### 16、hive底层与数据库交互原理
### 17、mapreduce的调度模式（题意模糊，可以理解为yarn的调度模式，也可以理解为mr的内部工作流程）
### 18、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。
### 19、搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。
### 20、varhadooppids用于做什么？
### 21、这会导致安全问题吗？
### 22、存储特点
### 23、请描述一下开发过程中如何对上面的程序进行性能分析，对性能分析进行优化的过程。
### 24、HBase写数据的原理是什么？
### 25、解释下hbase实时查询原理
### 26、腾讯面试题：给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？
### 27、hadoop-metrics.properties文件的作用是？
### 28、hbase中表的特点
### 29、RAM的溢出因子是？
### 30、combine出现在哪个过程
### 31、我们在开发分布式计算job的时候，是否可以去掉reduce阶段
### 32、mapred.job.tracker命令的作用？
### 33、datanode在什么情况下不会备份




## 全部答案，整理好了，直接下载吧

### 下载链接：[全部答案，整理好了](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin-2.png)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin-2.png)


## 最新，高清PDF：172份，7701页，最新整理

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/mst.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")
