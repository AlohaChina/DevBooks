# 大数据最新面试题及答案附答案汇总

### 其实，博主还整理了，更多大厂面试题，直接下载吧

### 下载链接：[高清172份，累计 7701 页大厂面试题  PDF](https://github.com/souyunku/DevBooks/blob/master/docs/index.md)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png)



### 1、Kafka与传统消息队列的区别

RabbitMQ吞吐量稍差Kafka，支持对消息可靠的传递，支持事务，不支持批量的操作，存储于内存或者磁盘

Kafka遵从一般的MQ结构，producer，broker，consumer，以consumer为中心，消费的消费信息保存在客户端consumer上，consumer根据消费的点，从broker批量pull数据；无消息确认机制

Kafka具有搞得吞吐量，内部采用消息的批量处理，数据的存储和获取是本地磁盘顺序批量操作，消息处理的效率高

Kafka 的broker支持主备模式

Kafka 负载均衡 Zookeeper方向

Kafka采用Zookeeper进行管理，可以注册topic到Zookeeper上，通过zoo的协调机制，生产者保存对应topic的broker消息，可以随机或者轮询发送到broker上；并且生产者可以基于予以定义指定分片，消息发送到broker的某分片上


### 2、HBase简单读写流程？

读：

找到要读数据的region所在的RegionServer，然后按照以下顺序进行读取：先去BlockCache读取，若BlockCache没有，则到Memstore读取，若Memstore中没有，则到HFile中去读。

写：

找到要写数据的region所在的RegionServer，然后先将数据写到WAL(Write-Ahead Logging，预写日志系统)中，然后再将数据写到Memstore等待刷新，回复客户端写入完成。


### 3、如何确认hadoop集群的健康状况

有完善的集群监控体系（ganglia，nagios）

Hdfs dfsadmin –report

Hdfs haadmin –getServiceState nn1


### 4、简答说一下hadoop的map-reduce编程模型

首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合

使用的是hadoop内置的数据类型，比如longwritable、text等

将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value在输出

之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getpartition方法来自定义分区规则

之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则

之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量

reduce task会通过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job


### 5、Master文件是否提供了多个入口？

是的你可以拥有多个Master文件接口。


### 6、有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。

还是典型的TOP K算法，解决方案如下：

方案1：

顺序读取10个文件，按照hash(query)的结果将query写入到另外10个文件（记为）中。这样新生成的文件每个的大小大约也1G（假设hash函数是随机的）。

找一台内存在2G左右的机器，依次对用hash_map(query, query_count)来统计每个query出现的次数。利用快速/堆/归并排序按照出现次数进行排序。将排序好的query和对应的query_cout输出到文件中。这样得到了10个排好序的文件（记为）。

对这10个文件进行归并排序（内排序与外排序相结合）。

方案2：

一般query的总量是有限的，只是重复的次数比较多而已，可能对于所有的query，一次性就可以加入到内存了。这样，我们就可以采用trie树/hash_map等直接来统计每个query出现的次数，然后按出现次数做快速/堆/归并排序就可以了。

方案3：

与方案1类似，但在做完hash，分成多个文件后，可以交给多个文件来处理，采用分布式的架构来处理（比如MapReduce），最后再进行合并。


### 7、如何重启Namenode？

点击stop-all.sh，再点击start-all.sh。

键入sudo hdfs（Enter），su-hdfs （Enter），etcinit.dha（Enter），及etcinit.dhadoop-0.20-namenode start（Enter）。


### 8、Hive中存放是什么？

表。

存的是和hdfs的映射关系，hive是逻辑上的数据仓库，实际操作的都是hdfs上的文件，HQL就是用sql语法来写的mr程序。


### 9、hadoop中常用的数据压缩算法

Lzo

Gzip

Default

Snapyy

如果要对数据进行压缩，最好是将原始数据转为SequenceFile 或者 Parquet File（spark）


### 10、hive使用版本

使用外部表，hive1.0 hive1.3


### 11、是否可以在Windows上运行Hadoop？
### 12、hbase内部机制是什么
### 13、spark调优
### 14、Spark的数据本地性有哪几种？
### 15、是否可以自行搭建Hadoop集群？
### 16、pig , latin , hive语法有什么不同
### 17、简述hadoop spark storm hive的特点及使用场景
### 18、Kafka 各组件介绍
### 19、map-reduce程序运行的时候会有什么比较常见的问题
### 20、如何访问hbase中的行
### 21、我们使用Ubuntu及Cloudera，那么我们该去哪里下载Hadoop，或者是默认就与Ubuntu一起安装？
### 22、RDD的弹性表现在哪几点？
### 23、hbase如何调优
### 24、hive底层与数据库交互原理
### 25、Hadoop性能调优？
### 26、hadoop的TextInputFormat作用是什么，如何自定义实现？
### 27、hadoop-env.sh是用于做什么的？
### 28、hadoop中，有哪些地方使用到了缓存机制，作用分别是什么？
### 29、hive与传统DB的区别
### 30、我们开发Job是否能去掉reduce阶段
### 31、KafkaUtils.createDstream 和 KafkaUtils.createDirectstream 区别
### 32、谈谈数据倾斜，如何发生的，并给出相应的解决办法




## 全部答案，整理好了，直接下载吧

### 下载链接：[全部答案，整理好了](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin-2.png)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin-2.png)


## 最新，高清PDF：172份，7701页，最新整理

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/mst.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")](https://www.souyunku.com/wp-content/uploads/weixin/githup-weixin.png "架构师专栏")
