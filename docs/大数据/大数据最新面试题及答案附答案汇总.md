# 大数据最新面试题及答案附答案汇总

### 其实，博主还整理了，更多大厂面试题，直接下载吧

### 下载链接：[高清172份，累计 7701 页大厂面试题  PDF](https://www.souyunku.com/?p=67)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/?p=67)



### 1、Kafka怎么保证消息不丢失机制

producer：分为同步模式与异步模式，同步模式效率低，异步模式效率高

Kafka的ack机制，在Kafkafa发送数据的时候，每次发送消息都会有一个确认反馈机制，确保消息正常能够被收到

同步模式：ack机制能够保数据的不丢失 ，不建议设置为0

producer.type=sync

request.required.acks=1

异步模式：通过buffer来进行控制数据的发送，时间阈值与消息数量阈值，如果buffer满了数据未发送，如果设置立即清理模式，风险很大，一定设置为阻塞模式

producer.type=sync

request.required.acks=1

queue.buffering.max.ms=5000

queue.buffering.max.messages=10000

queue.enqueue.timeout.ms=-1

batch.num.messages=200


### 2、使用zk来连接集群

./Kafka-console-consumer.sh --Zookeeper node01:2181,node2:2181,node3:2181 --from-beginning --topic test


### 3、介绍一下join操作优化经验？

join其实常见的就分为两类： map-side join 和 reduce-side join。当大表和小表join时，用map-side join能显著提高效率。将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。

备注：这个题目面试中非常非常大概率见到，务必搜索相关资料掌握，这里抛砖引玉。



### 4、收集日志的模型

分析业务，确定数据源，埋点收集日志，收集nginx日志，存储大hdfs，清洗数据，放入建模好的hive中，查询分析，结果导出，数据可视化

storm


### 5、hive内部表和外部表的区别

Hive 向内部表导入数据时，会将数据移动到数据仓库指向的路径；若是外部表，数据的具体存放目录由用户建表时指定

在删除表的时候，内部表的元数据和数据会被一起删除，

而外部表只删除元数据，不删除数据。

这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。


### 6、数据导入hive的方式

**1、** 本地数据导入hive

**2、** hdfs中导入hive

**3、** hbase导入hive

4查询方式导入hive


### 7、Hive与关系型数据库的关系？

没有关系，hive是数据仓库，不能和数据库一样进行实时的CURD操作。

是一次写入多次读取的操作，可以看成是ETL工具。


### 8、为什么会出现hadoop

采用多nn做成联邦，nn是独立的，nn之间不需要相互调用。NN是联合的，同属于一个联邦，作为DN的元数据公共存储


### 9、请说明hive中Sort By、Order By、Cluster By，Distribute By各代表什么意思？

order by：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。

sort by：不是全局排序，其在数据进入reducer前完成排序。

distribute by：按照指定的字段对数据进行划分输出到不同的reduce中。

cluster by：除了具有 distribute by 的功能外还兼具 sort by 的功能。


### 10、请写出以下的shell命令

**1、** 杀死一个job

**2、** 删除hdfs上的 /tmp/aaa目录

**3、** 加入一个新的存储节点和删除一个节点需要执行的命令

**4、** hadoop job –list 得到job的id，然后执 行 hadoop job -kill jobId就可以杀死一个指定jobId的job工作了。

**5、** hadoopfs -rmr /tmp/aaa

**6、** 增加一个新的节点在新的几点上执行

Hadoop  daemon.sh start  datanode

Hadooop daemon.sh  start  tasktracker/nodemanager

下线时，要在conf目录下的excludes文件中列出要下线的datanode机器主机名

然后在主节点中执行  hadoop  dfsadmin  -refreshnodes  à下线一个datanode

删除一个节点的时候，只需要在主节点执行

hadoop mradmin -refreshnodes —下线一个tasktracker/nodemanager


### 11、hive使用版本
### 12、Hadoop-env.sh文件当下的位置？
### 13、三个datanode中当有一个datanode出现错误时会怎样？
### 14、sqoop在导入数据到MySQL中，如何不重复导入数据，如果存在数据问题，sqoop如何处理？
### 15、有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。
### 16、MapReduce运行原理
### 17、如何解决spark数据倾斜
### 18、HBase简单读写流程？
### 19、storm特点
### 20、Hadoop需求什么样的网络？
### 21、假如Namenode中没有数据会怎么样？
### 22、描述一下hadoop中，有哪些地方使用到了缓存机制，作用分别是什么？
### 23、为什么hive的分区
### 24、hive如何优化
### 25、mapred.job.tracker命令的作用？
### 26、给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
### 27、HDFS读取文件的步骤
### 28、如何检查Namenode是否正常运行？
### 29、简要描述如何安装配置apache的一个开源hadoop，只描述即可，无需列出具体步骤，列出具体步骤更好。
### 30、在Hadoop_PID_DIR中，PID代表了什么？
### 31、为什么要用flume导入hdfs，hdfs的架构是怎样的？
### 32、创建topic：




## 全部答案，整理好了，直接下载吧

### 下载链接：[全部答案，整理好了](https://www.souyunku.com/?p=67)

### 一键直达：[https://www.souyunku.com/?p=67](https://www.souyunku.com/?p=67)


## 其他，高清PDF：172份，7701页，最新整理

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/mst.png "大厂面试题")](https://souyunku.lanzous.com/b0alp9b9g "大厂面试题")

## 关注公众号：架构师专栏，回复：“面试题”，即可

[![大厂面试题](https://www.souyunku.com/wp-content/uploads/weixin/jiagoushi.png "架构师专栏")](https://souyunku.lanzous.com/b0alp9b9g "架构师专栏")

## 关注公众号：架构师专栏，回复：“面试题”，即可
